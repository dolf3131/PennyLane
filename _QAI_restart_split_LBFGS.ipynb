{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca0869db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\miniconda3\\Lib\\site-packages\\pennylane\\__init__.py:196: RuntimeWarning: PennyLane is not yet compatible with JAX versions > 0.6.2. You have version 0.7.0 installed. Please downgrade JAX to 0.6.2 to avoid runtime errors using python -m pip install jax~=0.6.0 jaxlib~=0.6.0\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pennylane as qml\n",
    "from pennylane import numpy as np\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW, RAdam, LBFGS\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn import NLLLoss, BCEWithLogitsLoss\n",
    "from torch.utils.data import Subset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torch.nn import Module\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import Conv2d, Dropout2d, Linear, Dropout\n",
    "from torch import cat\n",
    "from tqdm import tqdm\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "142f9ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "transform = torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize((0.5,), (0.5,)),\n",
    "        #torchvision.transforms.RandomAffine(degrees=10, translate=(0.1, 0.1), scale=(0.9, 1.1)),\n",
    "        ])\n",
    "\n",
    "\n",
    "train_ds = torchvision.datasets.FashionMNIST(\n",
    "    \"./\", train=True, download=True,\n",
    "    transform=transform)\n",
    "\n",
    "test_ds = torchvision.datasets.FashionMNIST(\n",
    "    \"./\", train=False, download=True,\n",
    "    transform=transform)\n",
    "\n",
    "\n",
    "train_mask = (train_ds.targets == 0) | (train_ds.targets == 6)\n",
    "train_idx = torch.where(train_mask)[0]\n",
    "train_ds.targets[train_ds.targets == 6] = 1\n",
    "\n",
    "batch_size = 40\n",
    "\n",
    "binary_train_ds = Subset(train_ds, train_idx)\n",
    "train_loader = DataLoader(binary_train_ds, batch_size = batch_size, shuffle =True)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b301e6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BinaryClassifier(\n",
       "  (conv1): Conv2d(1, 8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (conv2): Conv2d(8, 25, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
       "  (dropout): Dropout2d(p=0.5, inplace=False)\n",
       "  (fc1): Linear(in_features=1225, out_features=36, bias=True)\n",
       "  (fn_dropout): Dropout(p=0.05, inplace=False)\n",
       "  (fc2): Linear(in_features=36, out_features=2, bias=True)\n",
       "  (fc3): Linear(in_features=2, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "class BinaryClassifier(Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = Conv2d(1, 8, kernel_size=5, padding=2)\n",
    "        self.conv2 = Conv2d(8, 25, kernel_size=5, padding=2)\n",
    "        self.dropout = Dropout2d()\n",
    "        self.fc1 = Linear(25 * 7 * 7, 36)\n",
    "        self.fn_dropout = Dropout(p=0.05)\n",
    "        self.fc2 = Linear(36, 2)\n",
    "        self.fc3 = Linear(2, 1)\n",
    "\n",
    "        # Quantum layer\n",
    "        self.num_q_wires = 2\n",
    "        self.q_device = qml.device(\"default.qubit\", wires=self.num_q_wires)\n",
    "        \n",
    "        self.num_q_layers = 2\n",
    "        qnn_params_size = (self.num_q_layers * 3 + 2) * self.num_q_wires\n",
    "        qnn_params_tensor = torch.empty(qnn_params_size, requires_grad=True)\n",
    "\n",
    "        # Kaiming 초기화의 gain 값 계산 (SiLU에 적합한 'leaky_relu'를 사용)\n",
    "        gain = torch.nn.init.calculate_gain('leaky_relu')\n",
    "        std = gain / qnn_params_size**0.5 # 텐서 크기를 이용한 표준 편차 계산\n",
    "        \n",
    "        # 균등 분포로 파라미터 초기화\n",
    "        torch.nn.init.uniform_(qnn_params_tensor, -std, std)\n",
    "\n",
    "        self.qnn_params = Parameter(qnn_params_tensor)\n",
    "\n",
    "        #@qml.qnode(self.q_device)\n",
    "        @qml.qnode(self.q_device, interface=\"torch\")\n",
    "        def circuit(x_batch):\n",
    "            param_idx = 0 \n",
    "            for layer in range(self.num_q_layers): # num_q_layers 만큼 반복\n",
    "                for i in range(self.num_q_wires):\n",
    "                    qml.H(wires=i)\n",
    "                    qml.RZ(2. * x_batch[:, i], wires=i)\n",
    "                    \n",
    "                for i in range(self.num_q_wires):\n",
    "                    qml.CRZ(2. * (torch.pi - x_batch[:, i]) * (torch.pi - x_batch[:, (i+1) % self.num_q_wires]), wires=[i, (i+1)%self.num_q_wires])\n",
    "\n",
    "                # EfficientSU2\n",
    "                for i in range(self.num_q_wires):\n",
    "                    qml.RY(self.qnn_params[param_idx], wires=i)\n",
    "                    param_idx += 1\n",
    "                    qml.RX(self.qnn_params[param_idx], wires=i)\n",
    "                    param_idx += 1\n",
    "                    #qml.Rot(self.qnn_params[param_idx], self.qnn_params[param_idx+1], torch.tensor(-torch.pi/2, dtype=torch.float64), wires=i)\n",
    "                    #param_idx += 2\n",
    "                    qml.Y(wires=i)\n",
    "\n",
    "                for i in range(self.num_q_wires):\n",
    "                    qml.CRX(self.qnn_params[param_idx], wires=[i, (i+1)%self.num_q_wires])\n",
    "                    param_idx += 1\n",
    "                    #qml.CNOT(wires=[i, (i+1)%self.num_q_wires])\n",
    "\n",
    "                \n",
    "                if layer == self.num_q_layers-1:\n",
    "                    for i in range(self.num_q_wires):\n",
    "                        qml.RY(self.qnn_params[param_idx], wires=i)\n",
    "                        param_idx += 1\n",
    "                        qml.RX(self.qnn_params[param_idx], wires=i)\n",
    "                        param_idx += 1\n",
    "                        #qml.Rot(self.qnn_params[param_idx], self.qnn_params[param_idx+1], torch.tensor(-torch.pi/2, dtype=torch.float64), wires=i)\n",
    "                        #param_idx += 2\n",
    "                        qml.Y(wires=i)\n",
    "            \n",
    "            #return qml.expval(self.obs) # qml.expval은 이제 (BATCH_SIZE,) 형태를 반환\n",
    "            return [qml.expval(qml.PauliZ(i)) for i in range(self.num_q_wires)]\n",
    "\n",
    "        self.qnn = circuit\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        #x = F.dropout(x, p=0.1)\n",
    "        x = self.fn_dropout(x)\n",
    "\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        #x = self.qnn(x).view(1,)  # apply QNN\n",
    "        qnn_out = self.qnn(x) \n",
    "        x = torch.stack(qnn_out, dim=-1).to(x.device)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        return F.log_softmax(cat((x, 1 - x), -1), -1)\n",
    "\n",
    "\n",
    "    # BinaryClassifier 클래스 내부에 추가\n",
    "    def forward_classical(self, x):\n",
    "        x = F.leaky_relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = F.leaky_relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = self.dropout(x, p=0.1)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "bc = BinaryClassifier()\n",
    "bc.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bb34611",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2400x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dummy_x = torch.tensor([[0.0, 0.0, 0.0, 0.0, 0.0, 0.0]], dtype=torch.float64).to(device)\n",
    "specs = qml.specs(bc.qnn)(dummy_x)\n",
    "total_params = sum(p.numel() for p in bc.parameters() if p.requires_grad)\n",
    "\n",
    "# 양자 회로 그리기\n",
    "fig, ax = qml.draw_mpl(bc.qnn)(dummy_x)\n",
    "plt.title(\"Modified Quantum Circuit (Custom Ansatz)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b25a57d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'device_name': 'default.qubit',\n",
      " 'diff_method': 'best',\n",
      " 'errors': {},\n",
      " 'gradient_fn': 'backprop',\n",
      " 'gradient_options': {},\n",
      " 'interface': 'torch',\n",
      " 'level': 'gradient',\n",
      " 'num_device_wires': 2,\n",
      " 'num_observables': 2,\n",
      " 'num_tape_wires': 2,\n",
      " 'num_trainable_params': 16,\n",
      " 'resources': Resources(num_wires=2,\n",
      "                        num_gates=34,\n",
      "                        gate_types=defaultdict(<class 'int'>,\n",
      "                                               {'CRX': 4,\n",
      "                                                'CRZ': 4,\n",
      "                                                'Hadamard': 4,\n",
      "                                                'PauliY': 6,\n",
      "                                                'RX': 6,\n",
      "                                                'RY': 6,\n",
      "                                                'RZ': 4}),\n",
      "                        gate_sizes=defaultdict(<class 'int'>,\n",
      "                                               {1: 26,\n",
      "                                                2: 8}),\n",
      "                        depth=21,\n",
      "                        shots=Shots(total_shots=None, shot_vector=()))}\n",
      "49462\n",
      "✅ 회로 제약 통과 — 학습을 계속합니다\n"
     ]
    }
   ],
   "source": [
    "pprint.pprint(specs, width=1)\n",
    "print(total_params)\n",
    "assert specs[\"num_tape_wires\"] <= 8,  \"❌ 큐빗 수 초과\"\n",
    "assert specs['resources'].depth <= 30, \"❌ 회로 깊이 초과\"\n",
    "assert specs[\"num_trainable_params\"]<= 60, \"❌ 학습 퀀텀 파라미터 수 초과\"\n",
    "assert total_params <= 50000, \"❌ 학습 전체 파라미터 수 초과\"\n",
    "\n",
    "print(\"✅ 회로 제약 통과 — 학습을 계속합니다\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb2af452",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(epochs, lr=0.0010, weight_decay=1e-6):\n",
    "    # 1. 고전 신경망 파라미터와 양자 신경망 파라미터 분리\n",
    "    classical_params = [\n",
    "        p for n, p in bc.named_parameters() if 'qnn_params' not in n\n",
    "    ]\n",
    "    qnn_params = [\n",
    "        p for n, p in bc.named_parameters() if 'qnn_params' in n\n",
    "    ]\n",
    "\n",
    "    # 2. 고전 신경망을 위한 옵티마이저\n",
    "    classical_optimizer = AdamW(classical_params, lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    # 3. 양자 신경망을 위한 옵티마이저 (학습률 감소)\n",
    "    qnn_optimizer = LBFGS(qnn_params, lr=lr)\n",
    "\n",
    "    # 스케줄러 (여기서는 고전 옵티마이저에만 적용)\n",
    "    scheduler = CosineAnnealingLR(classical_optimizer, T_max=epochs * len(train_loader), eta_min=1e-6)\n",
    "\n",
    "    loss_history = []\n",
    "    bc.train()\n",
    "\n",
    "    print(f\"Starting split training on {device}...\")\n",
    "    for epoch in range(epochs):\n",
    "        total_loss = []\n",
    "        \n",
    "        for data, target in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n",
    "            data = data.to(device)\n",
    "            target = target.to(device).flatten().long()\n",
    "\n",
    "\n",
    "            # 1. 고전 신경망 부분 학습\n",
    "            classical_optimizer.zero_grad(set_to_none=True)\n",
    "            output = bc(data)\n",
    "            loss = F.nll_loss(output, target)\n",
    "            loss.backward()\n",
    "            classical_optimizer.step()\n",
    "\n",
    "            # 2. 양자 신경망 부분 학습\n",
    "            def closure():\n",
    "            # LBFGS는 closure 함수 내에서 zero_grad를 호출해야 합니다.\n",
    "                qnn_optimizer.zero_grad()\n",
    "                \n",
    "                # 여기서 bc(data)를 다시 호출하여 새로운 순전파를 수행합니다.\n",
    "                output_qnn_step = bc(data.detach())\n",
    "                loss_qnn_step = F.nll_loss(output_qnn_step, target)\n",
    "                loss_qnn_step.backward()\n",
    "                \n",
    "                return loss_qnn_step\n",
    "\n",
    "            qnn_optimizer.step(closure)\n",
    "            #qnn_optimizer.zero_grad(set_to_none=True)\n",
    "            # 여기서는 손실을 다시 계산하여 qnn_params에 대한 기울기를 업데이트\n",
    "            #output_qnn_step = bc(data.detach())\n",
    "            #loss_qnn_step = F.nll_loss(output_qnn_step, target)\n",
    "            #loss_qnn_step.backward()\n",
    "            #qnn_optimizer.step()\n",
    "\n",
    "\n",
    "\n",
    "            scheduler.step()\n",
    "            total_loss.append(loss.item())\n",
    "        \n",
    "        avg_loss = sum(total_loss) / len(total_loss)\n",
    "        loss_history.append(avg_loss)\n",
    "        print(f\"Training [{100.0 * (epoch+1)/epochs:.0f}%] Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    return loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8bcd5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    print(f\"Starting inference on {device}...\")\n",
    "    bc.eval()\n",
    "\n",
    "    all_preds, all_targets = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(test_loader,\n",
    "                                desc=\"Inference\",\n",
    "                                total=len(test_loader),\n",
    "                                leave=False):\n",
    "            data, target = data.to(device), target.to(device).flatten().long() #target.to(device).float().unsqueeze(1)\n",
    "\n",
    "            logits = bc(data)\n",
    "\n",
    "            pred = logits.argmax(dim=-1)\n",
    "            #probabilities = torch.sigmoid(logits)\n",
    "            #pred = (probabilities >= 0.5).float().squeeze(1) \n",
    "\n",
    "            all_preds.append(pred.cpu())\n",
    "            all_targets.append(target.cpu())\n",
    "\n",
    "    y_pred = torch.cat(all_preds).numpy().astype(int)\n",
    "    y_true = torch.cat(all_targets).numpy().astype(int)\n",
    "\n",
    "    #y_pred = torch.cat(all_preds).numpy().flatten().astype(int) # .flatten() 추가\n",
    "    #y_true = torch.cat(all_targets).numpy().flatten().astype(int) # .flatten() 추가\n",
    "\n",
    "    # --- 8. 평가 및 결과 저장 ---\n",
    "    # 베이스라인 평가 로직을 따릅니다. y_true는 이제 10000개 길이를 가집니다.\n",
    "    test_mask = (y_true == 0) | (y_true == 6)\n",
    "\n",
    "    print(\"total samples:\", len(y_true))\n",
    "    print(\"target samples:\", test_mask.sum())\n",
    "\n",
    "    # 모델 결과가 1인 것을 6으로 변경 (제출 형식에 맞게)\n",
    "    y_pred_mapped = np.where(y_pred == 1, 6, y_pred)\n",
    "\n",
    "    # 정확도 계산은 0/6 라벨에 대해서만 수행\n",
    "    acc = (y_pred_mapped[test_mask] == y_true[test_mask]).mean()\n",
    "    print(f\"accuracy (labels 0/6 only): {acc:.4f}\")\n",
    "\n",
    "    now = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    y_pred_filename = f\"y_pred_{now}.csv\"\n",
    "\n",
    "    return y_pred_mapped, y_pred_filename, acc\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "71bf30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plt_loss_history(loss_history, n_repeats, epochs_per_repeat):\n",
    "    \"\"\"\n",
    "    학습 손실 이력을 시각화합니다.\n",
    "    Args:\n",
    "        loss_history (list): 에포크별 평균 손실 값 리스트\n",
    "        n_repeats (int): train_step 함수의 총 반복 횟수 (n)\n",
    "        epochs_per_repeat (int): 한 번의 train_step에서 에포크 수 (epochs)\n",
    "    \"\"\"\n",
    "    total_epochs = n_repeats * epochs_per_repeat\n",
    "    x_axis = range(1, total_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(x_axis, loss_history)\n",
    "    plt.title(\"Training Loss History\")\n",
    "    plt.xlabel(\"Total Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# 사용 예시\n",
    "# loss_history는 모든 반복(n)과 에포크(epochs)의 손실이 담긴 리스트\n",
    "# loss_history = [ ... ]\n",
    "\n",
    "# plt_loss_history(loss_history, n_repeats=15, epochs_per_repeat=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "977c8b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting split training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 300/300 [00:42<00:00,  7.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [10%] Loss: 0.4744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 300/300 [00:39<00:00,  7.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [20%] Loss: 0.3748\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 300/300 [00:34<00:00,  8.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [30%] Loss: 0.3305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 300/300 [00:31<00:00,  9.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [40%] Loss: 0.3061\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 300/300 [00:31<00:00,  9.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [50%] Loss: 0.2899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 300/300 [00:31<00:00,  9.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [60%] Loss: 0.2724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 300/300 [00:34<00:00,  8.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [70%] Loss: 0.2554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 300/300 [00:40<00:00,  7.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [80%] Loss: 0.2435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 300/300 [00:40<00:00,  7.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [90%] Loss: 0.2417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 300/300 [00:41<00:00,  7.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [100%] Loss: 0.2357\n",
      "Starting inference on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 10000\n",
      "target samples: 2000\n",
      "accuracy (labels 0/6 only): 0.8795\n",
      "Iteration 1: Result = 0.8795\n",
      "Starting split training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 300/300 [00:55<00:00,  5.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [10%] Loss: 0.2620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 300/300 [00:47<00:00,  6.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [20%] Loss: 0.2529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 300/300 [00:47<00:00,  6.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [30%] Loss: 0.2421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 300/300 [00:35<00:00,  8.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [40%] Loss: 0.2339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 300/300 [00:35<00:00,  8.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [50%] Loss: 0.2244\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 300/300 [00:37<00:00,  7.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [60%] Loss: 0.2045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 300/300 [00:36<00:00,  8.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [70%] Loss: 0.1990\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 300/300 [00:41<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [80%] Loss: 0.1895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 300/300 [00:46<00:00,  6.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [90%] Loss: 0.1835\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 300/300 [00:46<00:00,  6.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [100%] Loss: 0.1819\n",
      "Starting inference on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 10000\n",
      "target samples: 2000\n",
      "accuracy (labels 0/6 only): 0.8880\n",
      "Iteration 2: Result = 0.8880\n",
      "Starting split training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 300/300 [00:41<00:00,  7.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [10%] Loss: 0.2123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 300/300 [00:39<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [20%] Loss: 0.2065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 300/300 [00:39<00:00,  7.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [30%] Loss: 0.2002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 300/300 [00:39<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [40%] Loss: 0.1894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 300/300 [00:39<00:00,  7.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [50%] Loss: 0.1844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 300/300 [00:39<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [60%] Loss: 0.1662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 300/300 [00:39<00:00,  7.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [70%] Loss: 0.1577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 300/300 [00:43<00:00,  6.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [80%] Loss: 0.1556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 300/300 [00:43<00:00,  6.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [90%] Loss: 0.1526\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 300/300 [00:43<00:00,  6.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [100%] Loss: 0.1478\n",
      "Starting inference on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples: 10000\n",
      "target samples: 2000\n",
      "accuracy (labels 0/6 only): 0.8890\n",
      "Iteration 3: Result = 0.8890\n",
      "Starting split training on cuda...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 300/300 [00:40<00:00,  7.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [10%] Loss: 0.1734\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 300/300 [00:40<00:00,  7.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [20%] Loss: 0.1719\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 300/300 [00:39<00:00,  7.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [30%] Loss: 0.1659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 300/300 [00:41<00:00,  7.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [40%] Loss: 0.1565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 300/300 [00:42<00:00,  7.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training [50%] Loss: 0.1476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10:   8%|▊         | 25/300 [00:03<00:39,  7.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m result < \u001b[32m0.92\u001b[39m:\n\u001b[32m     11\u001b[39m     lr = \u001b[32m0.0015\u001b[39m * (\u001b[32m0.80\u001b[39m**n)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m     loss_history.extend(\u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1e-6\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m     13\u001b[39m     y_pred_mapped, y_pred_filename, result = test()\n\u001b[32m     14\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mIteration \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: Result = \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 50\u001b[39m, in \u001b[36mtrain_step\u001b[39m\u001b[34m(epochs, lr, weight_decay)\u001b[39m\n\u001b[32m     46\u001b[39m     loss_qnn_step.backward()\n\u001b[32m     48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_qnn_step\n\u001b[32m---> \u001b[39m\u001b[32m50\u001b[39m \u001b[43mqnn_optimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m#qnn_optimizer.zero_grad(set_to_none=True)\u001b[39;00m\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m# 여기서는 손실을 다시 계산하여 qnn_params에 대한 기울기를 업데이트\u001b[39;00m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m#output_qnn_step = bc(data.detach())\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m#loss_qnn_step = F.nll_loss(output_qnn_step, target)\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m#loss_qnn_step.backward()\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[38;5;66;03m#qnn_optimizer.step()\u001b[39;00m\n\u001b[32m     60\u001b[39m scheduler.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\Lib\\site-packages\\torch\\optim\\optimizer.py:487\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    482\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    483\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    484\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    485\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n\u001b[32m    490\u001b[39m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\Lib\\site-packages\\torch\\optim\\lbfgs.py:457\u001b[39m, in \u001b[36mLBFGS.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    452\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_iter != max_iter:\n\u001b[32m    453\u001b[39m     \u001b[38;5;66;03m# re-evaluate function only if not in last iteration\u001b[39;00m\n\u001b[32m    454\u001b[39m     \u001b[38;5;66;03m# the reason we do this: in a stochastic setting,\u001b[39;00m\n\u001b[32m    455\u001b[39m     \u001b[38;5;66;03m# no use to re-evaluate that function here\u001b[39;00m\n\u001b[32m    456\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m457\u001b[39m         loss = \u001b[38;5;28mfloat\u001b[39m(\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    458\u001b[39m     flat_grad = \u001b[38;5;28mself\u001b[39m._gather_flat_grad()\n\u001b[32m    459\u001b[39m     opt_cond = flat_grad.abs().max() <= tolerance_grad\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 46\u001b[39m, in \u001b[36mtrain_step.<locals>.closure\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m     44\u001b[39m output_qnn_step = bc(data.detach())\n\u001b[32m     45\u001b[39m loss_qnn_step = F.nll_loss(output_qnn_step, target)\n\u001b[32m---> \u001b[39m\u001b[32m46\u001b[39m \u001b[43mloss_qnn_step\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m loss_qnn_step\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\Lib\\site-packages\\torch\\_tensor.py:581\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    571\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    572\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    573\u001b[39m         Tensor.backward,\n\u001b[32m    574\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    579\u001b[39m         inputs=inputs,\n\u001b[32m    580\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m581\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    582\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    583\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\Lib\\site-packages\\torch\\autograd\\__init__.py:347\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    342\u001b[39m     retain_graph = create_graph\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    345\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    346\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m347\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    348\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    349\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    350\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    351\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    353\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    354\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\miniconda3\\Lib\\site-packages\\torch\\autograd\\graph.py:825\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    823\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    824\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    826\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    827\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    829\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "result = 0\n",
    "best_result = 0\n",
    "max_y_pred_filename = \"\"\n",
    "best_n = 0\n",
    "n = 0\n",
    "epochs = 10\n",
    "lr = 0.0015\n",
    "loss_history = []\n",
    "\n",
    "while result < 0.92:\n",
    "    lr = 0.0015 * (0.80**n)\n",
    "    loss_history.extend(train_step(epochs=epochs, lr=lr, weight_decay=batch_size * 1e-6))\n",
    "    y_pred_mapped, y_pred_filename, result = test()\n",
    "    print(f\"Iteration {n+1}: Result = {result:.4f}\")\n",
    "\n",
    "    if result > best_result:\n",
    "        best_result = result\n",
    "        best_n = n\n",
    "        max_y_pred_filename = y_pred_filename\n",
    "\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3873b2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best result: 0.8930 at iteration 15\n",
      "Prediction saved to y_pred_20250805_032755.csv\n"
     ]
    }
   ],
   "source": [
    "#plt_loss_history(loss_history, n, epochs)\n",
    "print(f\"Best result: {best_result:.4f} at iteration {best_n}\")\n",
    "# 전체 10000개 샘플에 대한 예측을 저장 (대회 제출 형식)\n",
    "np.savetxt(y_pred_filename, y_pred_mapped, fmt=\"%d\")\n",
    "print(f\"Prediction saved to {max_y_pred_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e2afe6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
